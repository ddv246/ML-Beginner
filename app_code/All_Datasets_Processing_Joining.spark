//
// DATA PROCESSING - GREATSCHOOLS DATA
//

val df = spark.read.option("header",true).csv("gs_data.csv")

// cast the Rating column to a numerical type
import org.apache.spark.sql.types.FloatType

val df2 = df.withColumn("gsRating", col("gsRating").cast(FloatType))

// non-linear function
import org.apache.spark.sql.functions.exp
val scored = df2.withColumn("score", (exp($"gsRating") / 2000.0 + $"gsRating") / 2.0)

val gb = scored.groupBy("zip").mean("score")

// this gives us the dataframe of average ratings
gb.show()

// saving
gb.withColumnRenamed("avg(score)","gsScore").write.save("gs_res.parquet")

//
// DATA PROCESSING - 311 DATA
//

val df = spark.read.option("header",false).csv("BDAD_Project/311_Cleaned/*")

// set header
val header = Seq("date", "complaint", "descriptor", "zip", "borough")
val requestsDF = df.toDF(header: _*)

// count complaints by zip code
val zipCount = requestsDF.groupBy("zip").count()

// count complaints by borough
val boroughCount = requestsDF.groupBy("borough").count()
boroughCount.show

// create non-linear function to apply to counts
import org.apache.spark.sql.functions.exp
def sCurve(count: Long): Double = {
val a = 2.5
val b = -0.0002
val c = -0.8
val k = 10.8
var score = (k / (1 + exp(a+(b*count)))) + c
score
}

// apply non-linear function to counts to obtain score for each zip code
import org.apache.spark.sql.functions.udf
val sCurveUDF = udf(sCurve _)
val zipCountScore = zipCount.withColumn("score", sCurveUDF('count))
val zipScore = zipCountScore.select($"zip", $"score")

// save dataframe as parquet files
zipScore.write.parquet("BDAD_Project/zipScore.parquet")

//
// DATA PROCESSING - NYC PROPERTY SALES DATA
//

//creating a dataframe from RDD

//Run the below line if reading from parquet file. If continuing from cleaning, please start from next line.
val df1 = spark.read.parquet("/Users/daffneyviswanath/Downloads/Property_Sales_Cleaned_final.parquet")

//If you did not save as a parquet file and directly converting to a Dataframe, please DO NOT run the previous line and run the below line
val df1 =spark.createDataFrame(rdd_filter_homebuildingsonly)

df1.count

df1.printSchema()

//median sale price in each borough
import org.apache.spark.sql.functions.{countDistinct,callUDF,lit}

df1.groupBy("BOROUGH")
.agg(callUDF("percentile_approx",$"SALE_PRICE",lit(0.5) )as ("Median Sale Price"))
.orderBy("BOROUGH").show(false)

//reviewing building class in each building category
df1.groupBy("BUILDING_CLASS_CATEGORY","BUILDING_CLASS_AS_OF_FINAL_ROLL")
.agg(countDistinct("BUILDING_CLASS_AS_OF_FINAL_ROLL"))
.orderBy("BUILDING_CLASS_CATEGORY")
.show(false)

//Average sale price wrt to each building class of a building category
import org.apache.spark.sql.functions.{min, max,avg,round}
df1.groupBy("BUILDING_CLASS_CATEGORY","BUILDING_CLASS_AS_OF_FINAL_ROLL")
.agg(round(avg("SALE_PRICE"),1) as ("Average Sale Price"))
.orderBy("BUILDING_CLASS_CATEGORY").show(false)

//Median sale price for each building class category by zipcode sorted by median price to check for outliers
df1.groupBy("zip_code","BUILDING_CLASS_CATEGORY")
.agg(callUDF("percentile_approx",$"SALE_PRICE",lit(0.5) )as ("Median"))
.orderBy($"Median".desc).show(false)

//Median price of a building category by zipcode 
import org.apache.spark.sql.functions.{sum, count, avg, expr,callUDF,lit,col,min,max}
val propertydf=df1.groupBy("zip_code","BUILDING_CLASS_CATEGORY")
.agg(callUDF("percentile_approx",$"SALE_PRICE",lit(0.5) )as ("Median"))
.orderBy($"ZIP_CODE")
.withColumnRenamed("BUILDING_CLASS_CATEGORY","buildingClass")
.withColumnRenamed("zip_code","zip")

propertydf.show(false)

//saving analysis to a dataframe
val property_class_df=propertydf.groupBy("zip").pivot("buildingClass").sum("median")

property_class_df.printSchema

property_class_df.count()

//
// DATA JOINING - ALL DATASETS
//

//Reading 311crimes dataframe 
val crimesdf = spark.read.parquet("/Users/daffneyviswanath/Downloads/zipScore.parquet")

crimesdf.printSchema()

//reading Schools Dataframe
val schoolsdf = spark.read.parquet("/Users/daffneyviswanath/Downloads/gs_res.parquet")

schoolsdf.printSchema()

crimesdf.count()

schoolsdf.count()

schoolsdf.select("zip").distinct.count

crimesdf.select("zip").distinct.count

val houseBuyingStatDF = property_class_df.join(schoolsdf,"zip").join(crimesdf,"zip")

houseBuyingStatDF.printSchema()

//Renaming columns
val houseBuyingStatRenamedDF=
houseBuyingStatDF.
withColumnRenamed("zip","ZIP_CODE").
withColumnRenamed("01 ONE FAMILY DWELLINGS","ONE_FAMILY_DWELLING").
withColumnRenamed("09 COOPS - WALKUP APARTMENTS","COOPS_WALKUP").
withColumnRenamed("10 COOPS - ELEVATOR APARTMENTS","COOPS_ELEVATOR").
withColumnRenamed("12 CONDOS - WALKUP APARTMENTS","CONDOS_WALKUP").
withColumnRenamed("13 CONDOS - ELEVATOR APARTMENTS","CONDOS_ELEVATOR").
withColumnRenamed("15 CONDOS - 2-10 UNIT RESIDENTIAL","CONDOS_2-10UNIT").
withColumnRenamed("gsScore","SCHOOL_SCORE").
withColumnRenamed("score","CRIME_SCORE")

%%dataframe
houseBuyingStatRenamedDF

houseBuyingStatRenamedDF.repartition(1).write.parquet("mergedDataset")






